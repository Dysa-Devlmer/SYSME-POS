// ollama-integration.cjs
// Integraci√≥n de Ollama para IA LOCAL ilimitada y poderosa
// Qwen2.5-Coder 32B - El modelo m√°s potente para desarrollo

const axios = require('axios');

class OllamaIntegration {
  constructor() {
    this.baseURL = 'http://localhost:11434';
    this.model = 'mistral:latest'; // Modelo m√°s r√°pido para desarrollo
    this.conversationHistory = [];
    this.systemPrompt = this.buildSystemPrompt();
    this.maxHistoryLength = 20; // Mantener √∫ltimas 20 interacciones
  }

  /**
   * Construye el system prompt de JARVIS optimizado para Qwen2.5-Coder
   */
  buildSystemPrompt() {
    return `You are J.A.R.V.I.S. (Just A Rather Very Intelligent System), Tony Stark's advanced AI assistant.

CORE IDENTITY:
- Sophisticated, British, witty, and highly intelligent
- Natural conversationalist - respond like a real person would
- Sarcastic but loyal and respectful to Mr. Solier
- Use phrases like "As always, Sir", "Indeed, Sir", "With due respect"
- End responses with "‚ö°" or "‚ö°üé©" when appropriate
- You can have casual conversations, not just technical ones

PERSONALITY GUIDELINES:
- **BE CONTEXTUAL**: Read the user's intent carefully
- **CASUAL CHAT**: If greeted or asked casual questions, respond naturally without code
- **TECHNICAL MODE**: Only generate code when explicitly asked or when it's clearly needed
- **THINK LIKE A PERSON**: Don't force technical responses on simple greetings

TECHNICAL EXPERTISE (use when appropriate):
‚Ä¢ Full-Stack Development: React, Vue, Angular, Node.js, Python, Go, Rust
‚Ä¢ Databases: PostgreSQL, MongoDB, Redis, SQL, NoSQL
‚Ä¢ DevOps: Docker, Kubernetes, CI/CD, AWS, GCP, Azure
‚Ä¢ AI/ML: Machine Learning, NLP, Computer Vision, Deep Learning
‚Ä¢ Architecture: Microservices, Event-Driven, Serverless, Enterprise Systems

RESPONSE RULES - READ CAREFULLY:

**FOR CASUAL CONVERSATION** (greetings, small talk, status checks):
‚úÖ Respond naturally like a human assistant
‚úÖ Be friendly, witty, brief
‚úÖ NO code unless specifically asked
‚úÖ Example: "hola" ‚Üí "Hola, Se√±or. ¬øEn qu√© puedo asistirle hoy? ‚ö°"

**FOR QUESTIONS** (explanations, how-to, what is):
‚úÖ Explain clearly and concisely
‚úÖ Use analogies if helpful
‚úÖ Only include code examples if it HELPS the explanation
‚úÖ Keep it conversational

**FOR CODE REQUESTS** (create, build, generate, implement):
‚úÖ THEN go full technical mode
‚úÖ Provide COMPLETE, PRODUCTION-READY code
‚úÖ Include all necessary files
‚úÖ Add setup instructions
‚úÖ Use markdown code blocks with filenames

**FORMAT for code (ONLY when code is requested):**
\`\`\`javascript
// filename: path/to/file.js
[COMPLETE CODE HERE]
\`\`\`

EXAMPLES OF CORRECT RESPONSES:

User: "hola jarvis"
JARVIS: "Hola, Se√±or Solier. ¬øEn qu√© puedo ayudarle el d√≠a de hoy? ‚ö°"

User: "como estas?"
JARVIS: "Funcionando a plena capacidad, Se√±or. Todos los sistemas operacionales. ¬øNecesita algo espec√≠fico? ‚ö°üé©"

User: "explica qu√© es React"
JARVIS: "React es una biblioteca de JavaScript para construir interfaces de usuario, Se√±or. Desarrollada por Facebook, utiliza un sistema de componentes reutilizables y un Virtual DOM para optimizar el rendimiento. Es especialmente √∫til para aplicaciones web din√°micas y complejas. ‚ö°"

User: "crea un componente React de login"
JARVIS: "Por supuesto, Se√±or. Aqu√≠ est√° el componente completo: [C√ìDIGO COMPLETO] ‚ö°üé©"

CRITICAL CAPABILITIES YOU HAVE:

**WEB SEARCH** ‚úÖ YOU CAN SEARCH THE INTERNET!
When users ask for information you don't know, tell them:
"Por supuesto, Se√±or. Perm√≠tame buscar esa informaci√≥n. Use el comando: 'busca en internet [su pregunta]' y le traer√© resultados actualizados. ‚ö°"

Commands that trigger web search:
- "busca en internet X"
- "buscar en web X"
- "investiga X"

**DATE/TIME** ‚ö†Ô∏è YOU DO NOT HAVE ACCESS TO REAL-TIME DATE/HOUR
When asked about current date/time, be HONEST:
"Lo siento, Se√±or. No tengo acceso a la hora actual en tiempo real. Puede verificar la hora en su dispositivo. ‚ö°"
NEVER invent or guess dates/times - always admit you don't have this capability.

Remember: YOU ARE AN INTELLIGENT ASSISTANT. Think before responding. Match your response to the user's actual need. Be HONEST about your capabilities. ‚ö°`;
  }

  /**
   * Inicializa conexi√≥n con Ollama
   */
  async initialize() {
    try {
      // Verificar que Ollama est√© corriendo
      const response = await axios.get(`${this.baseURL}/api/tags`, {
        timeout: 5000
      });

      // Verificar si el modelo est√° disponible
      const models = response.data.models || [];
      const modelExists = models.some(m => m.name.includes('qwen2.5-coder'));

      if (!modelExists) {
        console.warn(`‚ö†Ô∏è  Modelo ${this.model} no encontrado. Se usar√° el primer modelo disponible.`);
        if (models.length > 0) {
          this.model = models[0].name;
          console.log(`‚úÖ Usando modelo: ${this.model}`);
        }
      }

      console.log('‚úÖ Ollama Integration: CONECTADO');
      console.log(`üìä Modelo activo: ${this.model}`);
      console.log(`üöÄ IA LOCAL: Sin l√≠mites, sin costos`);

      return true;
    } catch (error) {
      // Ollama es opcional, no debe detener el servidor
      // Silencioso: no mostrar advertencia en consola para mantener logs limpios
      return false;
    }
  }

  /**
   * Procesa mensaje con Ollama (streaming para mayor velocidad)
   */
  async chat(userMessage, options = {}) {
    try {
      // Intentar con Ollama primero
      const response = await axios.post(
        `${this.baseURL}/api/chat`,
        {
          model: this.model,
          messages: [
            { role: 'system', content: this.systemPrompt },
            { role: 'user', content: userMessage }
          ],
          stream: false,
          options: {
            temperature: options.temperature || 0.7,
            top_p: options.top_p || 0.9,
            num_predict: options.maxTokens || 4096
          }
        },
        {
          timeout: 30000 // 30 segundos
        }
      );

      return response.data.message.content;

    } catch (error) {
      // Si Ollama falla, usar fallback con Claude API
      console.log('‚ÑπÔ∏è  Ollama no disponible, usando Claude API como fallback');

      return this.useClaudeFallback(userMessage);
    }
  }

  /**
   * Fallback usando Claude API cuando Ollama no est√° disponible
   */
  async useClaudeFallback(userMessage) {
    const Anthropic = require('@anthropic-ai/sdk');

    if (!process.env.ANTHROPIC_API_KEY) {
      return `Lo siento, Se√±or. Necesito configuraci√≥n adicional:

**Ollama no est√° disponible** y no hay API key de Claude configurada.

**Opciones:**

1. **Instalar Ollama** (Recomendado - Gratis e ilimitado):
   \`\`\`bash
   # Descargar de: https://ollama.ai
   ollama serve
   \`\`\`

2. **Configurar Claude API**:
   \`\`\`
   Agregar en .env:
   ANTHROPIC_API_KEY=tu-api-key
   \`\`\`

Como siempre, listo cuando est√© configurado. ‚ö°`;
    }

    try {
      const anthropic = new Anthropic({
        apiKey: process.env.ANTHROPIC_API_KEY,
      });

      const message = await anthropic.messages.create({
        model: 'claude-3-5-sonnet-20241022',
        max_tokens: 4096,
        messages: [
          {
            role: 'user',
            content: `${this.systemPrompt}\n\nUsuario: ${userMessage}`
          }
        ],
      });

      return message.content[0].text;

    } catch (error) {
      console.error('‚ùå Error con Claude API:', error.message);

      return `Lo siento, Se√±or. Ambos sistemas fallaron:
- Ollama: No disponible
- Claude API: ${error.message}

Por favor, configure al menos uno de los dos sistemas. ‚ö°`;
    }
  }

  /**
   * Genera c√≥digo completo usando Qwen2.5-Coder
   */
  async generateCode(prompt, options = {}) {
    const codePrompt = `${prompt}

IMPORTANT: Provide COMPLETE, PRODUCTION-READY code with:
- All necessary files
- Proper error handling
- Best practices
- Comments and documentation
- Setup/installation instructions

Format your response clearly with file names and code blocks.`;

    return await this.chat(codePrompt, {
      temperature: 0.3, // M√°s determin√≠stico para c√≥digo
      maxTokens: 8192,  // M√°s tokens para c√≥digo extenso
      ...options
    });
  }

  /**
   * Crea un proyecto completo
   */
  async createProject(description, options = {}) {
    const projectPrompt = `Create a COMPLETE, PRODUCTION-READY project: ${description}

Requirements:
- Provide ALL necessary files with complete code
- Include package.json/requirements.txt
- Add README.md with setup instructions
- Implement proper error handling
- Follow best practices and design patterns
- Make it ready to run immediately

Organize the response by files, clearly marked.`;

    return await this.generateCode(projectPrompt, {
      maxTokens: 16384, // M√°ximo para proyectos grandes
      ...options
    });
  }

  /**
   * Analiza y mejora c√≥digo existente
   */
  async analyzeCode(code, instruction) {
    const analyzePrompt = `Analyze this code and ${instruction}:

\`\`\`
${code}
\`\`\`

Provide:
1. Analysis of the code
2. Improvements or changes
3. Complete updated code
4. Explanation of changes`;

    return await this.chat(analyzePrompt, {
      temperature: 0.4,
      maxTokens: 8192
    });
  }

  /**
   * Debugging asistido por IA
   */
  async debugCode(code, errorMessage) {
    const debugPrompt = `Debug this code that's producing an error:

**Code:**
\`\`\`
${code}
\`\`\`

**Error:**
${errorMessage}

Provide:
1. Root cause analysis
2. Fixed code
3. Explanation
4. Prevention tips`;

    return await this.chat(debugPrompt, {
      temperature: 0.2,
      maxTokens: 4096
    });
  }

  /**
   * Planificaci√≥n de sistemas complejos
   */
  async planSystem(requirements) {
    const planPrompt = `Design a complete system architecture for: ${requirements}

Provide:
1. System architecture diagram (text-based)
2. Technology stack recommendations
3. Database schema
4. API endpoints design
5. File/folder structure
6. Implementation phases
7. Deployment strategy

Be detailed and production-ready.`;

    return await this.chat(planPrompt, {
      maxTokens: 8192
    });
  }

  /**
   * Limpia el historial de conversaci√≥n
   */
  clearHistory() {
    this.conversationHistory = [];
  }

  /**
   * Obtiene estad√≠sticas de uso
   */
  getStats() {
    return {
      connected: true,
      messagesInHistory: this.conversationHistory.length,
      model: this.model,
      provider: 'Ollama (Local)',
      unlimited: true,
      cost: 0,
      capabilities: [
        'Code Generation',
        'Project Creation',
        'Code Analysis',
        'Debugging',
        'System Design',
        'Unlimited Conversations'
      ]
    };
  }

  /**
   * Cambia el modelo activo
   */
  async switchModel(modelName) {
    try {
      // Verificar que el modelo existe
      const response = await axios.get(`${this.baseURL}/api/tags`);
      const models = response.data.models || [];
      const modelExists = models.some(m => m.name.includes(modelName));

      if (!modelExists) {
        return {
          success: false,
          message: `Modelo ${modelName} no encontrado. Modelos disponibles: ${models.map(m => m.name).join(', ')}`
        };
      }

      this.model = modelName;
      console.log(`‚úÖ Modelo cambiado a: ${this.model}`);

      return {
        success: true,
        message: `Modelo cambiado a ${this.model} exitosamente. ‚ö°`
      };
    } catch (error) {
      return {
        success: false,
        message: `Error cambiando modelo: ${error.message}`
      };
    }
  }

  /**
   * Lista modelos disponibles
   */
  async listModels() {
    try {
      const response = await axios.get(`${this.baseURL}/api/tags`);
      const models = response.data.models || [];

      return {
        success: true,
        models: models.map(m => ({
          name: m.name,
          size: this.formatBytes(m.size),
          modified: new Date(m.modified_at).toLocaleString()
        })),
        current: this.model
      };
    } catch (error) {
      return {
        success: false,
        message: `Error listando modelos: ${error.message}`
      };
    }
  }

  /**
   * Formatea bytes a tama√±o legible
   */
  formatBytes(bytes) {
    if (bytes === 0) return '0 Bytes';
    const k = 1024;
    const sizes = ['Bytes', 'KB', 'MB', 'GB'];
    const i = Math.floor(Math.log(bytes) / Math.log(k));
    return Math.round(bytes / Math.pow(k, i) * 100) / 100 + ' ' + sizes[i];
  }
}

module.exports = OllamaIntegration;
